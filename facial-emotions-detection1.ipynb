{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-11T11:12:11.090306Z","iopub.execute_input":"2023-07-11T11:12:11.090636Z","iopub.status.idle":"2023-07-11T11:12:11.116582Z","shell.execute_reply.started":"2023-07-11T11:12:11.090548Z","shell.execute_reply":"2023-07-11T11:12:11.115838Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function \nfrom __future__ import division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader\n\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:11.119940Z","iopub.execute_input":"2023-07-11T11:12:11.120162Z","iopub.status.idle":"2023-07-11T11:12:12.937919Z","shell.execute_reply.started":"2023-07-11T11:12:11.120135Z","shell.execute_reply":"2023-07-11T11:12:12.937075Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(\"PyTorch Version: \",torch.__version__)\nprint(\"Torchvision Version: \",torchvision.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:12.939433Z","iopub.execute_input":"2023-07-11T11:12:12.939722Z","iopub.status.idle":"2023-07-11T11:12:12.946406Z","shell.execute_reply.started":"2023-07-11T11:12:12.939684Z","shell.execute_reply":"2023-07-11T11:12:12.945535Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"PyTorch Version:  1.9.1\nTorchvision Version:  0.10.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Chọn một trong các model [resnet, alexnet, vgg16]\nmodel_name = \"resnet\"\n\n# Số các classes trong tập dữ liệu\nnum_classes = 8\n\n# Số các feature map trong mỗi một class\nnum_maps = 8\n\n# Batch size \nbatch_size = 10\n\n# Số các epochs để huấn luyện dữ liệu \nnum_epochs = 20\n\nfeature_extract = False\n\nmodel_savepath = 'models/'\n# Checkpoint\n'''checkpoint = torch.load('../input/fres101bat10num8d3/fres101bat10num8d3.pth') # checkpoint for continue training'''","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:12.947955Z","iopub.execute_input":"2023-07-11T11:12:12.948491Z","iopub.status.idle":"2023-07-11T11:12:12.962080Z","shell.execute_reply.started":"2023-07-11T11:12:12.948447Z","shell.execute_reply":"2023-07-11T11:12:12.961260Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"\"checkpoint = torch.load('../input/fres101bat10num8d3/fres101bat10num8d3.pth') # checkpoint for continue training\""},"metadata":{}}]},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n\n        # Mỗi epoch có hai giai đoạn train và test\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()  \n            else:\n                model.eval()   \n\n            running_loss = 0.0\n            running_corrects = 0\n            running_corrects_dec = 0\n\n            # Lặp qua data\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                \n                optimizer.zero_grad()\n\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    \n                    if is_inception and phase == 'train':\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = 0.5*loss1 + 0.5*loss2\n                    else:\n                        outputs1, outputs2 = model(inputs)\n                        loss1 = criterion(outputs1, labels)\n                        loss2 = criterion(outputs2, labels)\n                        loss = 0.5*loss1 + 0.5*loss2\n\n                    _, preds = torch.max(outputs2, 1)\n                    _, preds_dec = torch.max(outputs1, 1)\n\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        lr = optimizer.param_groups[0]['lr']\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                running_corrects_dec += torch.sum(preds_dec == labels.data)\n                \n            if phase == 'train':\n                scheduler_ft.step() \n                torch.save({\n                            'model_state_dict': model.state_dict(),\n                            'optimizer_state_dict': optimizer_ft.state_dict(),\n                            'epoch': epoch + 1,\n                            }, '/models' + str(epoch + 1) + '.pt')\n                \n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            epoch_acc_dec = running_corrects_dec.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f} Dec: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_acc_dec))\n\n            # deep copy the model\n            if phase == 'test' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                \n            if phase == 'test':\n                \n                val_acc_history.append(epoch_acc)\n            \n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    \n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history\n\nclass ResNetWSL(nn.Module):\n    \n    def __init__(self, model, num_classes, num_maps, pooling, pooling2):\n        super(ResNetWSL, self).__init__()\n        print(num_classes)\n        print(num_maps)\n        self.features = nn.Sequential(*list(model.children())[:-2])\n        # self.num_ftrs = model.fc.in_features\n\n        self.downconv = nn.Sequential(\n            nn.Conv2d(512, num_classes*num_maps, kernel_size=1, stride=1, padding=0, bias=True))\n        \n        self.GAP = nn.AvgPool2d(14)\n        self.GMP = nn.MaxPool2d(14)\n        self.spatial_pooling = pooling\n        self.spatial_pooling2 = pooling2\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, num_classes)\n            )\n    \n    def forward(self, x):\n        x = self.features(x)    #conv5\n        x_ori = x  \n        # detect branch\n        x = self.downconv(x) # qua 1x1\n        x_conv = x              \n        x = self.GMP(x)        \n        x = self.spatial_pooling(x) \n        x = x.view(x.size(0), -1) #v\n        # cls branch\n        x_conv = self.spatial_pooling(x_conv) \n        x_conv = x_conv * x.view(x.size(0),x.size(1),1,1) #vi*cimap\n        x_conv = self.spatial_pooling2(x_conv)  #M\n        x_conv_copy = x_conv  #M\n        for num in range(0,511):     #lặp qua mỗi feature trong M       \n            x_conv_copy = torch.cat((x_conv_copy, x_conv),1)\n        x_conv_copy = torch.mul(x_conv_copy,x_ori)  #U\n        #x_conv_copy1= x_conv_copy\n        x_conv_copy = torch.cat((x_ori,x_conv_copy),1) \n        #x_conv_copy = torch.cat((x_conv_copy1,x_conv_copy),1) #thêm \n        #x_conv_copy = torch.cat((x_conv_copy,x_ori),1) #thêm \n        x_conv_copy = self.GAP(x_conv_copy)   #d\n        x_conv_copy = x_conv_copy.view(x_conv_copy.size(0),-1)\n        x_conv_copy = self.classifier(x_conv_copy)\n        return x, x_conv_copy","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:12.965342Z","iopub.execute_input":"2023-07-11T11:12:12.965650Z","iopub.status.idle":"2023-07-11T11:12:12.999781Z","shell.execute_reply.started":"2023-07-11T11:12:12.965607Z","shell.execute_reply":"2023-07-11T11:12:12.998951Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch.autograd import Function, Variable\nclass ClassWisePoolFunction(Function):\n    @staticmethod\n    def forward(ctx, input, num_maps):\n        ctx.num_maps = num_maps\n        \n        batch_size, num_channels, h, w = input.size()\n\n        if num_channels % ctx.num_maps != 0:\n            print('Error in ClassWisePoolFunction. The number of channels has to be a multiple of the number of maps per class')\n            sys.exit(-1)\n\n        num_outputs = int(num_channels / ctx.num_maps)\n        x = input.view(batch_size, num_outputs, ctx.num_maps, h, w)\n        output = torch.sum(x, 2)\n        ctx.save_for_backward(input)\n        return output.view(batch_size, num_outputs, h, w) / ctx.num_maps\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        \n        batch_size, num_channels, h, w = input.size()\n        num_outputs = grad_output.size(1)\n\n        grad_input = grad_output.view(batch_size, num_outputs, 1, h, w).expand(batch_size, num_outputs, ctx.num_maps,\n                                                                               h, w).contiguous()\n        return grad_input.view(batch_size, num_channels, h, w), None\n\nclass ClassWisePool(nn.Module):\n    def __init__(self, num_maps):\n        super(ClassWisePool, self).__init__()\n        self.num_maps = num_maps\n\n    def forward(self, input):\n        return ClassWisePoolFunction.apply(input, self.num_maps)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:13.001425Z","iopub.execute_input":"2023-07-11T11:12:13.001731Z","iopub.status.idle":"2023-07-11T11:12:13.015596Z","shell.execute_reply.started":"2023-07-11T11:12:13.001689Z","shell.execute_reply":"2023-07-11T11:12:13.014592Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\ndef initialize_model(model_name, num_classes, num_maps, feature_extract, use_pretrained=True):\n    model_ft = None\n    input_size = 0\n    \n    model_ft = models.resnet18(pretrained=True)\n    set_parameter_requires_grad(model_ft, feature_extract)\n\n    pooling = nn.Sequential()\n    pooling.add_module('class_wise1', ClassWisePool(num_maps))\n    pooling2 = nn.Sequential()\n    pooling2.add_module('class_wise2', ClassWisePool(num_classes))\n    model_ft = ResNetWSL(model_ft, num_classes, num_maps, pooling, pooling2)\n        \n    input_size = 448\n    \n    return model_ft, input_size\n\n\nmodel_ft, input_size = initialize_model(model_name, num_classes, num_maps, feature_extract, use_pretrained=True)\n\n\n'''model_ft.load_state_dict(checkpoint['model_state_dict']) # load pretrained model'''\nprint(model_ft)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:13.017160Z","iopub.execute_input":"2023-07-11T11:12:13.017471Z","iopub.status.idle":"2023-07-11T11:12:13.644637Z","shell.execute_reply.started":"2023-07-11T11:12:13.017431Z","shell.execute_reply":"2023-07-11T11:12:13.643861Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/44.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87c447da13274bbc8b2bff7425792816"}},"metadata":{}},{"name":"stdout","text":"8\n8\nResNetWSL(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (5): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (6): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (7): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n  )\n  (downconv): Sequential(\n    (0): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (GAP): AvgPool2d(kernel_size=14, stride=14, padding=0)\n  (GMP): MaxPool2d(kernel_size=14, stride=14, padding=0, dilation=1, ceil_mode=False)\n  (spatial_pooling): Sequential(\n    (class_wise1): ClassWisePool()\n  )\n  (spatial_pooling2): Sequential(\n    (class_wise2): ClassWisePool()\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=1024, out_features=8, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        #transforms.RandomResizedCrop(input_size),\n        #transforms.RandomHorizontalFlip(),\n        transforms.Resize(input_size),\n        #transforms.CenterCrop(input_size),\n        #transforms.ColorJitter(brightness=1, contrast=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize(input_size),\n        #transforms.CenterCrop(input_size),\n        #transforms.ColorJitter(brightness=1, contrast=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\nprint(\"Initializing Datasets and Dataloaders...\")\n\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join('../input/fer2013/', x), data_transforms[x]) for x in ['train', 'test']}\n\n\ndataloaders_dict = {x: DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=2) for x in ['train', 'test']}\n\nclass_names = image_datasets['train'].classes\nprint(class_names)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:13.646205Z","iopub.execute_input":"2023-07-11T11:12:13.646496Z","iopub.status.idle":"2023-07-11T11:12:26.610679Z","shell.execute_reply.started":"2023-07-11T11:12:13.646457Z","shell.execute_reply":"2023-07-11T11:12:26.609875Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Initializing Datasets and Dataloaders...\n['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using GPU \", device)   \nmodel_ft = model_ft.to(device)\n\n\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n\noptimizer_ft = optim.SGD(params_to_update, lr=0.0001, momentum=0.9)\n'''optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict']) # load pretrained model's optimizer'''\nscheduler_ft = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-07-11T11:12:26.612047Z","iopub.execute_input":"2023-07-11T11:12:26.612339Z","iopub.status.idle":"2023-07-11T11:12:29.712903Z","shell.execute_reply.started":"2023-07-11T11:12:26.612308Z","shell.execute_reply":"2023-07-11T11:12:29.712218Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Using GPU  cuda:0\nParams to learn:\n\t features.0.weight\n\t features.1.weight\n\t features.1.bias\n\t features.4.0.conv1.weight\n\t features.4.0.bn1.weight\n\t features.4.0.bn1.bias\n\t features.4.0.conv2.weight\n\t features.4.0.bn2.weight\n\t features.4.0.bn2.bias\n\t features.4.1.conv1.weight\n\t features.4.1.bn1.weight\n\t features.4.1.bn1.bias\n\t features.4.1.conv2.weight\n\t features.4.1.bn2.weight\n\t features.4.1.bn2.bias\n\t features.5.0.conv1.weight\n\t features.5.0.bn1.weight\n\t features.5.0.bn1.bias\n\t features.5.0.conv2.weight\n\t features.5.0.bn2.weight\n\t features.5.0.bn2.bias\n\t features.5.0.downsample.0.weight\n\t features.5.0.downsample.1.weight\n\t features.5.0.downsample.1.bias\n\t features.5.1.conv1.weight\n\t features.5.1.bn1.weight\n\t features.5.1.bn1.bias\n\t features.5.1.conv2.weight\n\t features.5.1.bn2.weight\n\t features.5.1.bn2.bias\n\t features.6.0.conv1.weight\n\t features.6.0.bn1.weight\n\t features.6.0.bn1.bias\n\t features.6.0.conv2.weight\n\t features.6.0.bn2.weight\n\t features.6.0.bn2.bias\n\t features.6.0.downsample.0.weight\n\t features.6.0.downsample.1.weight\n\t features.6.0.downsample.1.bias\n\t features.6.1.conv1.weight\n\t features.6.1.bn1.weight\n\t features.6.1.bn1.bias\n\t features.6.1.conv2.weight\n\t features.6.1.bn2.weight\n\t features.6.1.bn2.bias\n\t features.7.0.conv1.weight\n\t features.7.0.bn1.weight\n\t features.7.0.bn1.bias\n\t features.7.0.conv2.weight\n\t features.7.0.bn2.weight\n\t features.7.0.bn2.bias\n\t features.7.0.downsample.0.weight\n\t features.7.0.downsample.1.weight\n\t features.7.0.downsample.1.bias\n\t features.7.1.conv1.weight\n\t features.7.1.bn1.weight\n\t features.7.1.bn1.bias\n\t features.7.1.conv2.weight\n\t features.7.1.bn2.weight\n\t features.7.1.bn2.bias\n\t downconv.0.weight\n\t downconv.0.bias\n\t classifier.0.weight\n\t classifier.0.bias\n","output_type":"stream"}]},{"cell_type":"code","source":"\ncriterion = nn.CrossEntropyLoss()\n\n\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft,\n                             num_epochs=num_epochs, is_inception=(model_name==\"inception\"))","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-07-11T11:12:29.716042Z","iopub.execute_input":"2023-07-11T11:12:29.716311Z","iopub.status.idle":"2023-07-11T13:07:56.431302Z","shell.execute_reply.started":"2023-07-11T11:12:29.716277Z","shell.execute_reply":"2023-07-11T13:07:56.429395Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Epoch 1/20\n----------\ntrain Loss: 1.4219 Acc: 0.4463 Dec: 0.4587\ntest Loss: 1.1970 Acc: 0.5386 Dec: 0.5518\n\nEpoch 2/20\n----------\ntrain Loss: 1.1244 Acc: 0.5754 Dec: 0.5837\ntest Loss: 1.0697 Acc: 0.6031 Dec: 0.5996\n\nEpoch 3/20\n----------\ntrain Loss: 0.9951 Acc: 0.6287 Dec: 0.6281\ntest Loss: 1.0684 Acc: 0.6095 Dec: 0.6027\n\nEpoch 4/20\n----------\ntrain Loss: 0.8992 Acc: 0.6672 Dec: 0.6646\ntest Loss: 1.0471 Acc: 0.6164 Dec: 0.6192\n\nEpoch 5/20\n----------\ntrain Loss: 0.8147 Acc: 0.7017 Dec: 0.6965\ntest Loss: 1.0571 Acc: 0.6314 Dec: 0.6306\n\nEpoch 6/20\n----------\ntrain Loss: 0.5831 Acc: 0.7940 Dec: 0.7787\ntest Loss: 1.1016 Acc: 0.6328 Dec: 0.6266\n\nEpoch 7/20\n----------\ntrain Loss: 0.4815 Acc: 0.8340 Dec: 0.8139\ntest Loss: 1.1530 Acc: 0.6433 Dec: 0.6353\n\nEpoch 8/20\n----------\ntrain Loss: 0.4246 Acc: 0.8575 Dec: 0.8318\ntest Loss: 1.3393 Acc: 0.6380 Dec: 0.6263\n\nEpoch 9/20\n----------\ntrain Loss: 0.3702 Acc: 0.8814 Dec: 0.8536\ntest Loss: 1.2969 Acc: 0.6330 Dec: 0.6271\n\nEpoch 10/20\n----------\ntrain Loss: 0.3143 Acc: 0.9024 Dec: 0.8743\ntest Loss: 1.4882 Acc: 0.6167 Dec: 0.6260\n\nEpoch 11/20\n----------\ntrain Loss: 0.1383 Acc: 0.9691 Dec: 0.9448\ntest Loss: 1.6331 Acc: 0.6544 Dec: 0.6434\n\nEpoch 12/20\n----------\ntrain Loss: 0.0652 Acc: 0.9919 Dec: 0.9777\ntest Loss: 1.8245 Acc: 0.6603 Dec: 0.6522\n\nEpoch 13/20\n----------\ntrain Loss: 0.0428 Acc: 0.9952 Dec: 0.9879\ntest Loss: 1.9309 Acc: 0.6600 Dec: 0.6491\n\nEpoch 14/20\n----------\ntrain Loss: 0.0302 Acc: 0.9965 Dec: 0.9929\ntest Loss: 1.9419 Acc: 0.6589 Dec: 0.6557\n\nEpoch 15/20\n----------\ntrain Loss: 0.0229 Acc: 0.9974 Dec: 0.9959\ntest Loss: 2.0893 Acc: 0.6666 Dec: 0.6588\n\nEpoch 16/20\n----------\ntrain Loss: 0.0164 Acc: 0.9976 Dec: 0.9971\ntest Loss: 2.0668 Acc: 0.6635 Dec: 0.6558\n\nEpoch 17/20\n----------\ntrain Loss: 0.0137 Acc: 0.9978 Dec: 0.9976\ntest Loss: 2.1750 Acc: 0.6675 Dec: 0.6623\n\nEpoch 18/20\n----------\ntrain Loss: 0.0125 Acc: 0.9979 Dec: 0.9977\ntest Loss: 2.2448 Acc: 0.6663 Dec: 0.6597\n\nEpoch 19/20\n----------\ntrain Loss: 0.0115 Acc: 0.9980 Dec: 0.9979\ntest Loss: 2.2640 Acc: 0.6650 Dec: 0.6590\n\nEpoch 20/20\n----------\ntrain Loss: 0.0110 Acc: 0.9977 Dec: 0.9978\ntest Loss: 2.2471 Acc: 0.6638 Dec: 0.6595\n\nTraining complete in 115m 27s\nBest val Acc: 0.667550\n","output_type":"stream"}]},{"cell_type":"code","source":"#torch.save({\n #           'model_state_dict': model_ft.state_dict(),\n  #          'optimizer_state_dict': optimizer_ft.state_dict(),\n   #         }, 'fres101bat10num8d3.pth')","metadata":{"execution":{"iopub.status.busy":"2023-07-11T13:07:56.433136Z","iopub.execute_input":"2023-07-11T13:07:56.433748Z","iopub.status.idle":"2023-07-11T13:07:56.438005Z","shell.execute_reply.started":"2023-07-11T13:07:56.433695Z","shell.execute_reply":"2023-07-11T13:07:56.437053Z"},"trusted":true},"execution_count":11,"outputs":[]}]}
